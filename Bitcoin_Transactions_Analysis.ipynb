{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMCwQ2cZ8FR0xgiH3OD9Vw9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fabrizi0rigodanz0/Airbnb_ML_Project/blob/main/Bitcoin_Transactions_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Analyzing Bitcoin Transactions with Pandas and Spark"
      ],
      "metadata": {
        "id": "ubsZ9gDlrcrP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "p_h2Hx-8EcwP",
        "tags": []
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "WITH_SPARK = IN_COLAB"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3fU9cEU7EcwP"
      },
      "source": [
        "## Install software\n",
        "\n",
        "This cell installs the software needed to run the project."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "KOzu05nlEcwP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "035770bc-d626-4c09-d453-627cd8ca32fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  openjdk-17-jre-headless\n",
            "Suggested packages:\n",
            "  openjdk-17-demo openjdk-17-source libnss-mdns fonts-dejavu-extra fonts-ipafont-gothic\n",
            "  fonts-ipafont-mincho fonts-wqy-microhei | fonts-wqy-zenhei fonts-indic\n",
            "The following NEW packages will be installed:\n",
            "  openjdk-17-jdk-headless openjdk-17-jre-headless\n",
            "0 upgraded, 2 newly installed, 0 to remove and 45 not upgraded.\n",
            "Need to get 119 MB of archives.\n",
            "After this operation, 271 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 openjdk-17-jre-headless amd64 17.0.10+7-1~22.04.1 [48.2 MB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 openjdk-17-jdk-headless amd64 17.0.10+7-1~22.04.1 [71.1 MB]\n",
            "Fetched 119 MB in 1s (81.8 MB/s)\n",
            "Selecting previously unselected package openjdk-17-jre-headless:amd64.\n",
            "(Reading database ... 121918 files and directories currently installed.)\n",
            "Preparing to unpack .../openjdk-17-jre-headless_17.0.10+7-1~22.04.1_amd64.deb ...\n",
            "Unpacking openjdk-17-jre-headless:amd64 (17.0.10+7-1~22.04.1) ...\n",
            "Selecting previously unselected package openjdk-17-jdk-headless:amd64.\n",
            "Preparing to unpack .../openjdk-17-jdk-headless_17.0.10+7-1~22.04.1_amd64.deb ...\n",
            "Unpacking openjdk-17-jdk-headless:amd64 (17.0.10+7-1~22.04.1) ...\n",
            "Setting up openjdk-17-jre-headless:amd64 (17.0.10+7-1~22.04.1) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-17-openjdk-amd64/bin/java to provide /usr/bin/java (java) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-17-openjdk-amd64/bin/jpackage to provide /usr/bin/jpackage (jpackage) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-17-openjdk-amd64/bin/keytool to provide /usr/bin/keytool (keytool) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-17-openjdk-amd64/bin/rmiregistry to provide /usr/bin/rmiregistry (rmiregistry) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-17-openjdk-amd64/lib/jexec to provide /usr/bin/jexec (jexec) in auto mode\n",
            "Setting up openjdk-17-jdk-headless:amd64 (17.0.10+7-1~22.04.1) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-17-openjdk-amd64/bin/jar to provide /usr/bin/jar (jar) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-17-openjdk-amd64/bin/jarsigner to provide /usr/bin/jarsigner (jarsigner) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-17-openjdk-amd64/bin/javac to provide /usr/bin/javac (javac) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-17-openjdk-amd64/bin/javadoc to provide /usr/bin/javadoc (javadoc) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-17-openjdk-amd64/bin/javap to provide /usr/bin/javap (javap) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-17-openjdk-amd64/bin/jcmd to provide /usr/bin/jcmd (jcmd) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-17-openjdk-amd64/bin/jdb to provide /usr/bin/jdb (jdb) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-17-openjdk-amd64/bin/jdeprscan to provide /usr/bin/jdeprscan (jdeprscan) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-17-openjdk-amd64/bin/jdeps to provide /usr/bin/jdeps (jdeps) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-17-openjdk-amd64/bin/jfr to provide /usr/bin/jfr (jfr) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-17-openjdk-amd64/bin/jimage to provide /usr/bin/jimage (jimage) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-17-openjdk-amd64/bin/jinfo to provide /usr/bin/jinfo (jinfo) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-17-openjdk-amd64/bin/jlink to provide /usr/bin/jlink (jlink) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-17-openjdk-amd64/bin/jmap to provide /usr/bin/jmap (jmap) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-17-openjdk-amd64/bin/jmod to provide /usr/bin/jmod (jmod) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-17-openjdk-amd64/bin/jps to provide /usr/bin/jps (jps) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-17-openjdk-amd64/bin/jrunscript to provide /usr/bin/jrunscript (jrunscript) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-17-openjdk-amd64/bin/jshell to provide /usr/bin/jshell (jshell) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-17-openjdk-amd64/bin/jstack to provide /usr/bin/jstack (jstack) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-17-openjdk-amd64/bin/jstat to provide /usr/bin/jstat (jstat) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-17-openjdk-amd64/bin/jstatd to provide /usr/bin/jstatd (jstatd) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-17-openjdk-amd64/bin/serialver to provide /usr/bin/serialver (serialver) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-17-openjdk-amd64/bin/jhsdb to provide /usr/bin/jhsdb (jhsdb) in auto mode\n",
            "E: Invalid operation wget\n",
            "Collecting pyspark==3.5.0\n",
            "  Downloading pyspark-3.5.0.tar.gz (316.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.9/316.9 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark==3.5.0) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.0-py2.py3-none-any.whl size=317425344 sha256=a7ec24c29265437fff09cc4d192383b75e0d37959f27bec08b3fd635ca61242a\n",
            "  Stored in directory: /root/.cache/pip/wheels/41/4e/10/c2cf2467f71c678cfc8a6b9ac9241e5e44a01940da8fbb17fc\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.0\n",
            "--2024-06-04 12:02:54--  https://repos.spark-packages.org/graphframes/graphframes/0.8.3-spark3.5-s_2.13/graphframes-0.8.3-spark3.5-s_2.13.jar\n",
            "Resolving repos.spark-packages.org (repos.spark-packages.org)... 13.226.34.35, 13.226.34.123, 13.226.34.14, ...\n",
            "Connecting to repos.spark-packages.org (repos.spark-packages.org)|13.226.34.35|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 251438 (246K) [binary/octet-stream]\n",
            "Saving to: ‘graphframes-0.8.3-spark3.5-s_2.13.jar’\n",
            "\n",
            "graphframes-0.8.3-s 100%[===================>] 245.54K  --.-KB/s    in 0.03s   \n",
            "\n",
            "2024-06-04 12:02:54 (8.75 MB/s) - ‘graphframes-0.8.3-spark3.5-s_2.13.jar’ saved [251438/251438]\n",
            "\n",
            "Requirement already satisfied: gdown in /usr/local/lib/python3.10/dist-packages (5.1.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown) (4.12.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown) (3.14.0)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.10/dist-packages (from gdown) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gdown) (4.66.4)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown) (2.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2024.2.2)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (1.7.1)\n"
          ]
        }
      ],
      "source": [
        "if( WITH_SPARK):\n",
        "    !apt-get install openjdk-17-jdk-headless\n",
        "    !apt-get wget\n",
        "    !pip install pyspark==3.5.0\n",
        "    !wget https://repos.spark-packages.org/graphframes/graphframes/0.8.3-spark3.5-s_2.13/graphframes-0.8.3-spark3.5-s_2.13.jar\n",
        "    os.environ['PYSPARK_SUBMIT_ARGS'] = '--jars graphframes-0.8.3-spark3.5-s_2.13.jar pyspark-shell'\n",
        "    os.environ[\"PYARROW_IGNORE_TIMEZONE\"] = \"1\"\n",
        "\n",
        "!pip install gdown\n",
        "!mkdir checkpoint\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IXHpSSnEEcwQ"
      },
      "source": [
        "## Setup\n",
        "\n",
        "The following cell will import the used packages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "7lyBwH2KEcwQ",
        "tags": []
      },
      "outputs": [],
      "source": [
        "from zipfile import ZipFile\n",
        "import time\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import ast\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import metrics\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if( WITH_SPARK):\n",
        "    import pyspark\n",
        "    import pyspark.pandas as ps\n",
        "    from pyspark.sql import SparkSession\n",
        "    from pyspark.sql.types import *\n",
        "    from pyspark.sql.functions import *\n",
        "    from pyspark.ml.linalg import Vectors\n",
        "    from pyspark.ml.clustering import KMeans\n",
        "    from pyspark.ml.evaluation import *\n",
        "    from pyspark.ml.feature import *"
      ],
      "metadata": {
        "id": "hjgoGVz6VDTI"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "anHDtopzEcwR"
      },
      "source": [
        "## Dataset\n",
        "\n",
        "In this project, you will be asked to perform analysis of [Bitcoin](https://en.wikipedia.org/wiki/Bitcoin) transactions, a cryptocurrency where transactions are stored in blocks of a blockchain - for the purpose of this project, a blockchain can be seen as a list of blocks, and each block has a list of transactions.\n",
        "\n",
        "The provided data files include a list of transactions performed in Bitcoin. The list of transactions is continuous and ordered in time, being a subset of all transactions performed in Bitcoin. A transaction transfers currency from one or more source addresses to one or more destination addresses.\n",
        "\n",
        "The datasets are a transformation form the data provided at [https://www.kaggle.com/shiheyingzhe/datasets](https://www.kaggle.com/shiheyingzhe/datasets).\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jX14oN5OEibP"
      },
      "source": [
        "The data sets are available in the following link: [https://drive.google.com/drive/folders/1WSJTm5nfy64uOc648TJ-SI1CHj5MbiH_?usp=sharing](https://drive.google.com/drive/folders/1WSJTm5nfy64uOc648TJ-SI1CHj5MbiH_?usp=sharing). For running locally download the smallest file and at least another one and store it in directory data. For running in Google Colab, you should access the link and Add Shortcut to your Drive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "OrdPJwdJHaUt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c59f9a2e-404b-489d-d0cf-5c0e6bdf25d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "0-68732.csv.gz\t      260001-270000.csv.gz  320001-329999.csv.gz  360001-364000.csv.gz\n",
            "190001-209999.csv.gz  270001-280000.csv.gz  330000-337000.csv.gz  364001-367000.csv.gz\n",
            "210000-224000.csv.gz  280001-290000.csv.gz  337001-343000.csv.gz  367001-369999.csv.gz\n",
            "224001-234000.csv.gz  290001-300000.csv.gz  343001-349000.csv.gz  btc_price.csv\n",
            "234001-247000.csv.gz  300001-310000.csv.gz  349001-354000.csv.gz  labels\n",
            "247001-260000.csv.gz  310001-320000.csv.gz  354001-360000.csv.gz\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# If you added the shortcut to your drive, the file should appear in this directory\n",
        "# If not, you need to explore from directory /content/drive\n",
        "!ls /content/drive/MyDrive/sbe2324ada"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "I_hp6Rv2EcwR",
        "tags": []
      },
      "outputs": [],
      "source": [
        "DATASET_DIR = \"bitcoin/archive1\"\n",
        "# FOR running in Colab\n",
        "DATASET_DIR = \"/content/drive/MyDrive/sbe2324ada\"\n",
        "\n",
        "# Small dataset\n",
        "TXDATA_FILE = \"0-68732.csv.gz\"\n",
        "\n",
        "# Larger dataset\n",
        "TXDATA_FILE2 = \"224001-234000.csv.gz\"\n",
        "\n",
        "# Bitcoin price\n",
        "BTCPRICE_FILE = \"btc_price.csv\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5U6Xf8luEcwR"
      },
      "source": [
        "### Bitcoin transactions\n",
        "\n",
        "Each transactions CSV file has five columns, with the following contents:\n",
        "\n",
        "* **TxId**: a string of format \"block:counter\" - this can be used to identify univocally a transaction.\n",
        "* **Height**: the block in which the transaction is recorded.\n",
        "* **Input**: list of source addresses.\n",
        "* **Output**: list of destination addresses; when the list included multiple values, it includes the value transferred for each account. The following example represented two destination addresses, the first receiving *0.01* and the second *49.99*:  \n",
        "```[['1Fmjwt8172FZT5XdKnzTUcEEzc1T2MCg2a', '0.01'], ['1AbHNFdKJeVL8FRZyRZoiTzG9VCmzLrtvm', '49.99']]```\n",
        "* **Sum**: a list with a single element that is the sum of the value transferred.\n",
        "* **Time**: the time of the transaction.\n",
        "\n",
        "\n",
        "The following cells print basic infomation about the transaction files stored in the directory. The first uses Pandas, while the next two use Pandas interface for Spark."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a_8B38HhEcwS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e841e5b7-3ea1-48d3-e64b-28dcd0238976"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DUMP BASIC INFO FOR EACH FILE\n",
            "FILENAME : 0-68732.csv.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pyspark/pandas/utils.py:1016: PandasAPIOnSparkAdviceWarning: If `index_col` is not specified for `read_csv`, the default index is attached which can cause additional overhead.\n",
            "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of transactions : 14832\n",
            "First block : 546\n",
            "Last block : 68732\n",
            "First date : 2009-01-15 06:08:20\n",
            "Last date : 2010-07-17 16:54:44\n",
            "FILENAME : 190001-209999.csv.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pyspark/pandas/utils.py:1016: PandasAPIOnSparkAdviceWarning: If `index_col` is not specified for `read_csv`, the default index is attached which can cause additional overhead.\n",
            "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of transactions : 4183123\n",
            "First block : 190001\n",
            "Last block : 209999\n",
            "First date : 2012-07-19 09:43:01\n",
            "Last date : 2014-03-12 22:49:29\n",
            "FILENAME : 210000-224000.csv.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pyspark/pandas/utils.py:1016: PandasAPIOnSparkAdviceWarning: If `index_col` is not specified for `read_csv`, the default index is attached which can cause additional overhead.\n",
            "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of transactions : 4372368\n",
            "First block : 210000\n",
            "Last block : 224000\n",
            "First date : 2012-09-06 14:25:44\n",
            "Last date : 2014-03-13 09:37:24\n",
            "FILENAME : 224001-234000.csv.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pyspark/pandas/utils.py:1016: PandasAPIOnSparkAdviceWarning: If `index_col` is not specified for `read_csv`, the default index is attached which can cause additional overhead.\n",
            "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of transactions : 3268711\n",
            "First block : 224001\n",
            "Last block : 234000\n",
            "First date : 2012-08-24 18:14:12\n",
            "Last date : 2013-05-01 03:47:20\n",
            "FILENAME : 234001-247000.csv.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pyspark/pandas/utils.py:1016: PandasAPIOnSparkAdviceWarning: If `index_col` is not specified for `read_csv`, the default index is attached which can cause additional overhead.\n",
            "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of transactions : 3755673\n",
            "First block : 234001\n",
            "Last block : 247000\n",
            "First date : 1972-09-04 19:15:29\n",
            "Last date : 2083-06-22 23:32:50\n",
            "FILENAME : 247001-260000.csv.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pyspark/pandas/utils.py:1016: PandasAPIOnSparkAdviceWarning: If `index_col` is not specified for `read_csv`, the default index is attached which can cause additional overhead.\n",
            "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of transactions : 3605859\n",
            "First block : 247001\n",
            "Last block : 260000\n",
            "First date : 2013-07-15 11:31:03\n",
            "Last date : 2014-03-16 01:06:12\n",
            "FILENAME : 260001-270000.csv.gz\n"
          ]
        }
      ],
      "source": [
        "# Dumps basic infomation using Spark Pandas API\n",
        "spark = SparkSession \\\n",
        "    .builder \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .appName(\"Group project\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Dumps basic infomation using native Pandas\n",
        "print( \"DUMP BASIC INFO FOR EACH FILE\")\n",
        "for filename in os.listdir(DATASET_DIR) :\n",
        "    if( not filename.endswith(\".csv.gz\")):\n",
        "        continue;\n",
        "    print( \"FILENAME : \" + filename )\n",
        "    path = os.path.join(DATASET_DIR, filename)\n",
        "    df = ps.read_csv( path,compression=\"gzip\")\n",
        "    print( \"Number of transactions : \" + str(df.shape[0]))\n",
        "    print( \"First block : \" + str(df[\"Height\"].min()))\n",
        "    print( \"Last block : \" + str(df[\"Height\"].max()))\n",
        "    print( \"First date : \" + str(df[\"Time\"].min()))\n",
        "    print( \"Last date : \" + str(df[\"Time\"].max()))\n",
        "    del df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sg2mgIfpEcwR"
      },
      "outputs": [],
      "source": [
        "# Dumps basic infomation using native Pandas\n",
        "print( \"DUMP BASIC INFO FOR EACH FILE\")\n",
        "for filename in os.listdir(DATASET_DIR) :\n",
        "    if( not filename.endswith(\".csv.gz\")):\n",
        "        continue;\n",
        "    print( \"FILENAME : \" + filename )\n",
        "    date_cols = [\"Time\"]\n",
        "    path = os.path.join(DATASET_DIR, filename)\n",
        "    df = pd.read_csv( path,compression=\"gzip\",parse_dates=date_cols)\n",
        "    print( \"Number of transactions : \" + str(df.shape[0]))\n",
        "    print( \"First block : \" + str(df[\"Height\"].min()))\n",
        "    print( \"Last block : \" + str(df[\"Height\"].max()))\n",
        "    print( \"First date : \" + str(df[\"Time\"].min()))\n",
        "    print( \"Last date : \" + str(df[\"Time\"].max()))\n",
        "    del df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2sk0A6iIEcwS"
      },
      "outputs": [],
      "source": [
        "# Dumps basic infomation using Spark Dataframe load\n",
        "spark = SparkSession \\\n",
        "    .builder \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .appName(\"Group project\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Dumps basic infomation using native Pandas\n",
        "print( \"DUMP BASIC INFO FOR EACH FILE\")\n",
        "for filename in os.listdir(DATASET_DIR) :\n",
        "    if( not filename.endswith(\".csv.gz\")):\n",
        "        continue;\n",
        "    print( \"FILENAME : \" + filename )\n",
        "    path = os.path.join(DATASET_DIR, filename)\n",
        "    mySchema = StructType([\n",
        "        StructField(\"TxId\", StringType()),\n",
        "        StructField(\"Height\", IntegerType()),\n",
        "        StructField(\"Input\", StringType()),\n",
        "        StructField(\"Output\", StringType()),\n",
        "        StructField(\"Sum\", StringType()),\n",
        "        StructField(\"Time\", TimestampType()),\n",
        "    ])\n",
        "\n",
        "    dataset = spark.read.load(path, format=\"csv\", compression=\"gzip\",\n",
        "                         sep=\",\", schema=mySchema, header=\"true\")\n",
        "    dataset.createOrReplaceTempView(\"data\")\n",
        "\n",
        "    statistics = spark.sql( \"\"\"SELECT COUNT( *) AS num_transactions,\n",
        "                                  MIN( Height) AS first_block,\n",
        "                                  MAX( Height) AS last_block,\n",
        "                                  MIN( Time) AS first_time,\n",
        "                                  MAX( Time) AS last_time\n",
        "                                  FROM data\"\"\")\n",
        "    statistics.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RUgHRRgcEcwS"
      },
      "source": [
        "### Prepocessing a dataset file\n",
        "\n",
        "For simplifying the processing, we will split the transactions dataframe into the following dataframes:\n",
        "\n",
        "* **inputDF**, includes the index of the transaction, one input address, the sum and the time.\n",
        "* **outputDF**, includes the index of the transaction, one output address, the output value, the sum and the time.\n",
        "* **transactionDF**, includes the index of the transaction, the sum and the time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q9oE0zvhEcwS"
      },
      "outputs": [],
      "source": [
        "date_cols = [\"Time\"]\n",
        "path = os.path.join(DATASET_DIR, TXDATA_FILE2)\n",
        "df = pd.read_csv( path,compression=\"gzip\",parse_dates=date_cols)\n",
        "\n",
        "df = df.sample(frac=1/10, random_state=42)\n",
        "\n",
        "df['Input'] = df['Input'].apply(ast.literal_eval)\n",
        "df['Output'] = df['Output'].apply(ast.literal_eval)\n",
        "\n",
        "inputDF = df[[\"TxId\",\"Height\",\"Input\",\"Sum\",\"Time\"]].explode(\"Input\")\n",
        "print(inputDF.dtypes)\n",
        "\n",
        "outputDF = df[[\"TxId\",\"Height\",\"Output\",\"Sum\",\"Time\"]].explode(\"Output\")\n",
        "outputDF[[\"Output\",\"Value\"]] = outputDF[\"Output\"].to_list()\n",
        "print(outputDF.dtypes)\n",
        "\n",
        "transactionDF = df[[\"TxId\",\"Height\",\"Sum\",\"Time\"]]\n",
        "print(transactionDF.dtypes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USy7wswvEcwS"
      },
      "source": [
        "We repeat the same computation in Spark SQL, creating views with the same names."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark = SparkSession \\\n",
        "    .builder \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .appName(\"Group project\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "path = os.path.join(DATASET_DIR, TXDATA_FILE2)\n",
        "mySchema = StructType([\n",
        "    StructField(\"TxId\", StringType()),\n",
        "    StructField(\"Height\", IntegerType()),\n",
        "    StructField(\"Input\", StringType()),\n",
        "    StructField(\"Output\", StringType()),\n",
        "    StructField(\"Sum\", FloatType()),\n",
        "    StructField(\"Time\", TimestampType()),\n",
        "])\n",
        "\n",
        "dataset = spark.read.load(path, format=\"csv\", compression=\"gzip\",\n",
        "                         sep=\",\", schema=mySchema, header=\"true\")\n",
        "dataset = dataset.withColumn(\"Input\",split(regexp_replace(\"Input\",\"[\\[\\s\\]]\",\"\"),\"\\,\"))\n",
        "dataset = dataset.withColumn(\"Output\",split(\"Output\",\"\\]\\, \\[\"))\n",
        "dataset = dataset.sample(fraction=1/10, seed=42)\n",
        "dataset.createOrReplaceTempView(\"data\")\n",
        "\n",
        "inputSDF = spark.sql( \"\"\"SELECT txid, height, EXPLODE(input) AS input, sum, time\n",
        "                                  FROM data\"\"\")\n",
        "inputSDF.createOrReplaceTempView(\"input\")\n",
        "inputSDF.printSchema()\n",
        "\n",
        "\n",
        "outputSDF = spark.sql( \"\"\"SELECT txid, height, EXPLODE(output) AS output, sum, time\n",
        "                                      FROM data\"\"\")\n",
        "outputSDF = outputSDF.withColumn(\"output\",split(regexp_replace(\"output\",\"[\\[\\]]\",\"\"),\"\\,\"))\n",
        "outputSDF = outputSDF.withColumn(\"value\",expr(\"CAST(output[1] AS FLOAT)\"))\n",
        "outputSDF = outputSDF.withColumn(\"output\",expr(\"output[0]\"))\n",
        "outputSDF.createOrReplaceTempView(\"output\")\n",
        "outputSDF.printSchema()\n",
        "\n",
        "transactionSDF = spark.sql( \"\"\"SELECT txid, height, sum, time\n",
        "                                  FROM data\"\"\")\n",
        "transactionSDF.createOrReplaceTempView(\"transaction\")\n",
        "transactionSDF.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2aUjXcLdS5CO",
        "outputId": "55505d73-4c4f-484a-a213-1207997eaf39"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- txid: string (nullable = true)\n",
            " |-- height: integer (nullable = true)\n",
            " |-- input: string (nullable = false)\n",
            " |-- sum: float (nullable = true)\n",
            " |-- time: timestamp (nullable = true)\n",
            "\n",
            "root\n",
            " |-- txid: string (nullable = true)\n",
            " |-- height: integer (nullable = true)\n",
            " |-- output: string (nullable = true)\n",
            " |-- sum: float (nullable = true)\n",
            " |-- time: timestamp (nullable = true)\n",
            " |-- value: float (nullable = true)\n",
            "\n",
            "root\n",
            " |-- txid: string (nullable = true)\n",
            " |-- height: integer (nullable = true)\n",
            " |-- sum: float (nullable = true)\n",
            " |-- time: timestamp (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oszh-5CgVBLl"
      },
      "source": [
        "### Bitcoin price\n",
        "\n",
        "This dataset has, for each day, the USD price of one bitcoin. The file is ```btc_price.csv```.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4KjclYa5VBLl",
        "outputId": "20985966-27c4-4b4d-9ad2-fe8c6de31f70"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "           Date    Price\n",
            "0    2009-01-15      0.1\n",
            "1    2009-01-16      0.1\n",
            "2    2009-01-17      0.1\n",
            "3    2009-01-18      0.1\n",
            "4    2009-01-19      0.1\n",
            "...         ...      ...\n",
            "5417 2023-11-15  37874.9\n",
            "5418 2023-11-16  36161.2\n",
            "5419 2023-11-17  36595.4\n",
            "5420 2023-11-18  36568.6\n",
            "5421 2023-11-19  36792.1\n",
            "\n",
            "[5422 rows x 2 columns]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Date     datetime64[ns]\n",
              "Price           float64\n",
              "dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "date_cols = [\"Date\"]\n",
        "path = os.path.join(DATASET_DIR, BTCPRICE_FILE)\n",
        "\n",
        "priceDF = pd.read_csv( path,parse_dates=date_cols)\n",
        "print(priceDF)\n",
        "priceDF.dtypes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "khF0tTu4VBLm"
      },
      "source": [
        "### Bitcoin tags\n",
        "\n",
        "This dataset includes a set of files with different types of addresses.\n",
        "Currently there are several files for different types of entities.\n",
        "\n",
        "A new dataset file, easier to process, will be made available soon in the same directory.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EMxHiZm-2TMF"
      },
      "source": [
        "## Exercise 0\n",
        "\n",
        "This example computes, for each address, the number of transactions in which the address has been involved.\n",
        "\n",
        "We have the code using Spark and Pandas, printing the time for doing the computation.\n",
        "**Draw some conclusions** by comparing the time for performing the computation using Spark and Pandas, and also when using the different datasets - **Make sure you run each cell more than once and write down the last value**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XGTMYKxV7n-U"
      },
      "source": [
        "### Pandas code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WD1vz2Ha7ray",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1406052f-a7f3-4ff4-c0f3-b18a97700fdf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                     cnt_in   cnt_out       cnt\n",
            "1dice8EMZmqKvrGE4Qc9bUFf9PX3xaYDp  156000.0  154285.0  310285.0\n",
            "1dice97ECuByXAvqXpaYzSaQuPVvrtmz6  145342.0  145236.0  290578.0\n",
            "1dice9wcMu5hLF4g81u8nioL5mmSHTApw   89521.0   89427.0  178948.0\n",
            "1dice7fUkz5h4z2wPc1wLMPWgB5mDwKDx   69890.0   69741.0  139631.0\n",
            "1dice7W2AicHosf5EL3GFDUVga7TgtPFn   63969.0   63892.0  127861.0\n",
            "1VayNert3x1KzbpzMGt2qdqrAThiRovi8   52129.0   52297.0  104426.0\n",
            "1diceDCd27Cc22HV3qPNZKwGnZ8QwhLTc   50376.0   50032.0  100408.0\n",
            "1dice1e6pdhLzzWQq7yMidf6j8eAg7pkY   44032.0   44241.0   88273.0\n",
            "1dice6YgEVBf88erBFra9BHf6ZMoyvG88   39163.0   39059.0   78222.0\n",
            "1dice7EYzJag7SxkdKXLr8Jn14WUb3Cf1   37015.0   36795.0   73810.0\n",
            "Runtime = 235.94536519050598\n"
          ]
        }
      ],
      "source": [
        "start_time = time.time()\n",
        "\n",
        "date_cols = [\"Time\"]\n",
        "path = os.path.join(DATASET_DIR, TXDATA_FILE2)\n",
        "df = pd.read_csv( path,compression=\"gzip\",parse_dates=date_cols)\n",
        "\n",
        "df['Input'] = df['Input'].apply(ast.literal_eval)\n",
        "df['Output'] = df['Output'].apply(ast.literal_eval)\n",
        "\n",
        "inputDF = df[[\"TxId\",\"Height\",\"Input\",\"Sum\",\"Time\"]].explode(\"Input\")\n",
        "\n",
        "outputDF = df[[\"TxId\",\"Height\",\"Output\",\"Sum\",\"Time\"]].explode(\"Output\")\n",
        "#outputDF[[\"Output\",\"Value\"]] = outputDF[\"Output\"].to_list()\n",
        "outputDF = outputDF.dropna(subset=['Output'])  # Remove NaNs if any\n",
        "outputDF['Output'], outputDF['Value'] = zip(*outputDF['Output'].apply(lambda x: x if len(x) == 2 else (None, None)))\n",
        "\n",
        "addr_in = inputDF[['Input','TxId']].groupby('Input').count().rename(columns={'TxId':\"cnt_in\"})\n",
        "addr_out = outputDF[['Output','TxId']].groupby('Output').count().rename(columns={'TxId':\"cnt_out\"})\n",
        "addr=addr_in.join(addr_out,how=\"outer\").fillna(0)\n",
        "addr[\"cnt\"] = addr[\"cnt_in\"] + addr[\"cnt_out\"]\n",
        "print(addr.nlargest(10,\"cnt\"))\n",
        "\n",
        "\n",
        "end_time = time.time()\n",
        "\n",
        "print( \"Runtime = \" + str(end_time - start_time))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FAMxFNXT7rwu"
      },
      "source": [
        "### Results (Pandas)\n",
        "\n",
        "The time to process the small dataset was : **0.8033947944641113** seconds.\n",
        "\n",
        "The time to process the large dataset was : **235.94536519050598** seconds."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ASS443Y77Qo2"
      },
      "source": [
        "### Spark SQL code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RryvCjhdEcwT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "523b4e17-ade0-4a46-a78a-bac0f2ac8524"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------------------------+------+------+-------+\n",
            "|addr                               |cnt   |cnt_in|cnt_out|\n",
            "+-----------------------------------+------+------+-------+\n",
            "|'1dice8EMZmqKvrGE4Qc9bUFf9PX3xaYDp'|310285|156000|154285 |\n",
            "|'1dice97ECuByXAvqXpaYzSaQuPVvrtmz6'|290578|145342|145236 |\n",
            "|'1dice9wcMu5hLF4g81u8nioL5mmSHTApw'|178948|89521 |89427  |\n",
            "|'1dice7fUkz5h4z2wPc1wLMPWgB5mDwKDx'|139631|69890 |69741  |\n",
            "|'1dice7W2AicHosf5EL3GFDUVga7TgtPFn'|127861|63969 |63892  |\n",
            "|'1VayNert3x1KzbpzMGt2qdqrAThiRovi8'|104426|52129 |52297  |\n",
            "|'1diceDCd27Cc22HV3qPNZKwGnZ8QwhLTc'|100408|50376 |50032  |\n",
            "|'1dice1e6pdhLzzWQq7yMidf6j8eAg7pkY'|88273 |44032 |44241  |\n",
            "|'1dice6YgEVBf88erBFra9BHf6ZMoyvG88'|78222 |39163 |39059  |\n",
            "|'1dice7EYzJag7SxkdKXLr8Jn14WUb3Cf1'|73810 |37015 |36795  |\n",
            "+-----------------------------------+------+------+-------+\n",
            "\n",
            "Runtime = 109.76431059837341\n"
          ]
        }
      ],
      "source": [
        "spark = SparkSession \\\n",
        "    .builder \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .appName(\"Group project\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "path = os.path.join(DATASET_DIR, TXDATA_FILE2)\n",
        "mySchema = StructType([\n",
        "    StructField(\"TxId\", StringType()),\n",
        "    StructField(\"Height\", IntegerType()),\n",
        "    StructField(\"Input\", StringType()),\n",
        "    StructField(\"Output\", StringType()),\n",
        "    StructField(\"Sum\", FloatType()),\n",
        "    StructField(\"Time\", TimestampType()),\n",
        "])\n",
        "\n",
        "dataset = spark.read.load(path, format=\"csv\", compression=\"gzip\",\n",
        "                         sep=\",\", schema=mySchema, header=\"true\")\n",
        "dataset = dataset.withColumn(\"Input\",split(regexp_replace(\"Input\",\"[\\[\\s\\]]\",\"\"),\"\\,\"))\n",
        "dataset = dataset.withColumn(\"Output\",split(\"Output\",\"\\]\\, \\[\"))\n",
        "dataset.createOrReplaceTempView(\"data\")\n",
        "\n",
        "inputSDF = spark.sql( \"\"\"SELECT txid, height, EXPLODE(input) AS input, sum, time\n",
        "                                  FROM data\"\"\")\n",
        "inputSDF.createOrReplaceTempView(\"input\")\n",
        "\n",
        "outputSDF = spark.sql( \"\"\"SELECT txid, height, EXPLODE(output) AS output, sum, time\n",
        "                                      FROM data\"\"\")\n",
        "outputSDF = outputSDF.withColumn(\"output\",split(regexp_replace(\"output\",\"[\\[\\]]\",\"\"),\"\\,\"))\n",
        "outputSDF = outputSDF.withColumn(\"value\",expr(\"CAST(output[1] AS FLOAT)\"))\n",
        "outputSDF = outputSDF.withColumn(\"output\",expr(\"output[0]\"))\n",
        "outputSDF.createOrReplaceTempView(\"output\")\n",
        "\n",
        "spark.sql( \"\"\"SELECT i.addr, cnt_in + cnt_out AS cnt, cnt_in, cnt_out FROM (\n",
        "              (SELECT input AS addr, COUNT(*) AS cnt_in FROM input GROUP BY input) i FULL OUTER JOIN\n",
        "              (SELECT output AS addr, COUNT(*) AS cnt_out FROM output GROUP BY output) o ON i.addr = o.addr)\n",
        "                ORDER BY cnt DESC\n",
        "                LIMIT 10\"\"\").show(truncate=False)\n",
        "end_time = time.time()\n",
        "\n",
        "print( \"Runtime = \" + str(end_time - start_time))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aS1p9t707Uvu"
      },
      "source": [
        "### Results (Spark)\n",
        "\n",
        "The time to process the small dataset was : **1.6324033737182617** seconds.\n",
        "\n",
        "The time to process the large dataset was : **109.18929481506348** seconds.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QWFmcsKV7mtn"
      },
      "source": [
        "### Discussion:\n",
        "\n",
        "PySpark is ideal for large databases due to its distributed computing and in-memory processing, enabling fast, parallel processing of big data across multiple machines. This is the reason why it is much faster than pandas in doing calculations on the larger dataset.\n",
        "However the parallel processing metod utilized is not efficient when dealing with small dataset as transferring data from a machine to another is an expensive task in terms of time.\n",
        "For this reason Pandas is better for smaller datasets. It operates efficiently on a single machine with an in-memory data structure, offering fast data manipulation with a simple syntax.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kvp6FprNVBLp"
      },
      "source": [
        "## Exercise 1 [3 points]\n",
        "\n",
        "This first group consists in computing some statistics on the infomration available in the dataset.\n",
        "\n",
        "The smallest dataset might lead to strange results for some statistics. Use it while you are developing you project, but if you run into strange results, just use a larger one.\n",
        "\n",
        "\n"
      ]
    }
  ]
}